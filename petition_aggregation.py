# -*- coding: utf-8 -*-
"""Petition_Aggregation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ey98kcoTaws6J8ODvq1EF5GMWcjWVtD-
"""

#!pip install praw

import bs4 as bs
from urllib.request import urlopen, Request
import pandas as pd
import re
from datetime import datetime
import praw

#Create a header to prevent 404 error
headers={'User-Agent': 'Mozilla/5.0'}

"""The Petition Site"""

#Request a webpage
source = ('https://www.thepetitionsite.com/animal-rights/2/#hottest')
req = Request(url = source, headers = headers)
html = urlopen(req).read()

#Create a parse tree
soup = bs.BeautifulSoup(html, 'lxml')

#Get Petition Titles
titles1 = []
for tag in soup.findAll('div', {'class':'title'}): # get all divs with 'title' class
    titles1.append(tag.get_text()) #output text-based results to array
titlesdf = pd.DataFrame(data = titles1) #output to pandas df

#Get Petition Links
urlarray = []
for url in soup.findAll('a', {"class": "sign_button"}):
    urlarray.append("https://www.thepetitionsite.com" + url.get('href')) #add 'https://www.thepetitionsite.com/' to the href and append to array
links1 = pd.DataFrame(data = urlarray)

#Join DataFrames and rename columns
result = pd.concat([titlesdf, links1], axis = 1)
result.columns = ['Titles', 'Links'] 
print(result)

"""Lady Freethinker"""

#Request a webpage
source = ('https://ladyfreethinker.org/category/speakout-petitions/')
req = Request(url = source, headers = headers)
html = urlopen(req).read()

#Create a parse tree
soup = bs.BeautifulSoup(html, 'lxml')

#Get Petition Titles
titles2 = []
for tag in soup.findAll('h2', {'class':'post-title entry-title'}): #get all h2 tags with the 'post-title entry-title' class
    titles2.append(tag.get_text()) #output to array
titlesdf2 = pd.DataFrame(data = titles2) #output to pandas df
titlesdf2.head()

#Get Petition Links
links2 = []
for url in soup.findAll('a', {"class": "et-accent-color"}):
    links2.append(url.get('href'))
linksdf2 = pd.DataFrame(data = links2)

#Join DataFrames and rename columns
result2 = pd.concat([titlesdf2, linksdf2], axis = 1)
result2.columns = ['Titles', 'Links'] 
print(result2)

"""GoPetition"""

#Request a webpage
source = ('https://www.gopetition.com/latest-petitions/Animal-rights')
req = Request(url = source, headers = headers)
html = urlopen(req).read()

#Create a parse tree
soup = bs.BeautifulSoup(html, 'lxml')

#Get Petition Titles
titles3 = []
for tag in soup.findAll('div', {'class':'head'}): #get all div tags with the 'head' class
    titles3.append(tag.get_text()) #output to array

#Get Petition Links
links3 = []
for url in soup.findAll('a', {"class": "petitions-block-item"}):
    links3.append("https://www.gopetition.com" + url.get('href'))

#Add to dataframe
result3 = pd.DataFrame({'Titles': titles3,'Links': links3}) 
print(result3)

"""Change.org"""

#Request a webpage
source = ('https://www.change.org/t/animal-welfare-6?source_location=topic_page')
req = Request(url = source, headers = headers)
html = urlopen(req).read()

#Create a parse tree
soup = bs.BeautifulSoup(html, 'lxml')

#Get Petition Titles
titles4 = []
for tag in soup.findAll('h4', {'class':'mtn mbxxs'}): #get all div tags with the 'hide-overflow' class
    titles4.append(tag.get_text()) #output to array

#Get Petition Links
links4 = []
for url in soup.findAll('a', {"class": "title clearfix link-block"}):
    links4.append("https://www.change.org" + url.get('href'))

#Add to dataframe
result4 = pd.DataFrame({'Titles': titles4,'Links': links4}) 
print(result4)

"""Animal Petitions: https://animalpetitions.org/

SPCAI: https://www.spcai.org/take-action/sign-our-petitions

Concat all DFs
"""

final_result = pd.concat([result,result2,result3,result4]) 
final_result = final_result.replace(r'\\n',' ', regex=True) 
final_result.replace(r'\s', ' ', regex = True, inplace = True)
#final_result.Titles = final_result.Titles.replace(r'\\n',' ', regex=True, inplace=True) 
print(final_result)

"""Upload to SQL db and remove columns that existed in previous saves"""

import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base

#%load_ext sql
#%sql postgres://faqbhbor:61hkFz976AN_aX-hQ8NDdjpxL4FKiOB2@ruby.db.elephantsql.com:5432/faqbhbor 
        
engine = create_engine('postgres://faqbhbor:61hkFz976AN_aX-hQ8NDdjpxL4FKiOB2@ruby.db.elephantsql.com:5432/faqbhbor') 
print (engine.table_names())

#Collect the previous dataframe
previous_table = pd.io.sql.read_sql('select "Titles", "Links" from all_petitions', engine)
print(previous_table)

#Merge the dataframes and remove old rows
final_result = final_result.merge(previous_table, indicator='i', how='outer').query('i == "left_only"').drop('i', 1)
final_result.head()

# Push DF to 'animal_petition' table
final_result.to_sql('animal_petition', con=engine, if_exists='replace')
final_result.to_sql('all_petitions', con=engine, if_exists='append')

# Commented out IPython magic to ensure Python compatibility.
# %%sql
# 
# SELECT * FROM animal_petition LIMIT 5

# Close the connection
engine.dispose()

"""Merge tables and only keep new data

Upload to Reddit
"""

# Reddit api login
r = praw.Reddit(client_id='xCVUMgJhFz63vg',
                client_secret='',
                username='ironmagnesiumzinc',
                password='',
                user_agent='<console:HAPPY:1.0>')

# The subreddit to post to
subreddit = r.subreddit('testingground4bots')

# Specify Params
date = datetime.today().strftime('%Y-%m-%d')
title = 'Petitions (%s)' % date

# Modify Markdown dataframe - INSERT THIS CELL INTO LIVE SCRIPT

# Remove most emojis (not sure if this works)
final_result.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))

# Make a clickable URL (Format links as Markdown Hyperlinks)
df = pd.DataFrame(final_result)

def make_hyperlink(Titles, Links):
    return '[{}]({})'.format(Titles, Links)

final_result['Links'] = df.apply(lambda x: make_hyperlink(x['Titles'], x['Links']), axis=1)
final_result.drop('Titles', axis=1, inplace=True)

# Save as Markdown
text = final_result.to_markdown()
print(text)

# Post
subreddit.submit(title = title, selftext = text, send_replies = False)

# DeprecationWarning: Reddit will check for validation on all posts around May-June 2020. 
# It is recommended to check for validation by setting reddit.validate_on_submit to True.